{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask and XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask and XGBoost can work together to train gradient boosted trees in parallel. This notebook shows how.\n",
    "\n",
    "XGBoost provides a powerful prediction framework. It wins Kaggle contests and is popular in industry because it has good performance (i.e., high accuracy models) and can be easily interpreted (i.e., it's easy to find the important features from a XGBoost model).\n",
    "\n",
    "The goals of this notebook are to show Dask and XGBoost working together, and explain a little bit of what they do together.  Briefly:\n",
    "\n",
    "1.  Dask provides distributed dataframes for data access, cleaning, and pre-processing\n",
    "2.  XGBoost provides distributed training and prediction\n",
    "3.  Dask is able to set up XGBoost in distributed mode, hand it data, and let it do the training easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\" width=\"30%\" alt=\"Dask logo\"> <img src=\"https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/logo-m/xgboost.png\" width=\"25%\" alt=\"Dask logo\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a synthetic data with realistic sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "num_samples, num_features = 100000, 20\n",
    "\n",
    "X = da.random.normal(size=(num_samples, num_features),\n",
    "                     chunks=num_samples // 100)\n",
    "w_star = da.random.uniform(size=num_features,\n",
    "                           chunks=num_features)**3\n",
    "y = da.sign(X @ w_star)\n",
    "\n",
    "# for binary:logistic objective, only [0, 1] labels allowed\n",
    "y = (y + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separate our data into a training set and testing set, which will allow for good evaluation after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with XGBoost\n",
    "\n",
    "Now, let's try to do something with this data using [dask-xgboost][dxgb].\n",
    "\n",
    "[dxgb]:https://github.com/dask/dask-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_xgboost\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dask-xgboost is a small wrapper around xgboost, and will behave the same as xgboost.\n",
    "\n",
    "During training Dask will take care of loading, cleaning, and pre-processing the data. XGBoost will leverage their own distributed training system using all the workers that Dask has available. Dask sets XGBoost up, gives XGBoost data and lets XGBoost do it's training in the background using all the workers Dask has available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'objective': 'binary:logistic', 'nround': 1000, \n",
    "          'max_depth': 5, 'eta': 0.01, 'subsample': 0.5, \n",
    "          'min_child_weight': 0.5}\n",
    "\n",
    "bst = dask_xgboost.train(client, params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bst` object is a regular `xgboost.Booster` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means all the methods mentioned in the [XGBoost documentation][2] are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot feature importance\n",
    "\n",
    "For example we use XGBoost features to plot the importance of features.\n",
    "\n",
    "[2]:https://xgboost.readthedocs.io/en/latest/python/python_intro.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask\n",
    "\n",
    "w_star = dask.compute(w_star)[0]\n",
    "idx_top = np.argsort(w_star)[-7:]\n",
    "top = w_star[idx_top]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12, 8), ncols=2)\n",
    "\n",
    "axs[0] = xgboost.plot_importance(bst, ax=axs[0], height=0.8, max_num_features=9)\n",
    "\n",
    "axs[0].grid(False, axis=\"y\")\n",
    "axs[0].set_title('Estimated feature importance')\n",
    "\n",
    "df = pd.DataFrame({'feature_importance': top}, index=idx_top)\n",
    "df.plot.bar(ax=axs[1])\n",
    "axs[1].set_title('Ground truth feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this example that XGBoost does a good job at feature *support recovery*, or figuring out which features are important. Notice that in both plots, feature `9` shows up in both plots (in the left plot as `f9`).  The top 5 estimated most important features are *actually* the top 5 most important features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curve\n",
    "\n",
    "\n",
    "We can also use Scikit-Learn to plot the [Receiver Operating Characteristic (ROC) curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) to see how well our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = dask_xgboost.predict(client, bst, X_test).persist()\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "fpr, tpr, _ = roc_curve(y_test, y_hat)\n",
    "ax.plot(fpr, tpr, lw=3,\n",
    "        label='ROC Curve (area = {:.2f})'.format(auc(fpr, tpr)))\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "ax.set(\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    title=\"ROC Curve\",\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    ")\n",
    "ax.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Receiver Operating Characteristic (ROC) curve tells how well our classifier is doing. We can tell it's doing well by how far it bends the upper-left. A perfect classifier would be in the upper-left corner, and a random classifier would follow the horizontal line.\n",
    "\n",
    "The area under this curve is `area = 0.89`. This tells us the probability that our classifier will predict correctly for a randomly chosen instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "* XGBoost documentation: https://xgboost.readthedocs.io/en/latest/python/python_intro.html#\n",
    "* Dask-XGBoost documentation: http://dask-ml.readthedocs.io/en/latest/xgboost.html\n",
    "* A blogpost on dask-xgboost http://matthewrocklin.com/blog/work/2017/03/28/dask-xgboost\n",
    "* Similar example with real world dataset: https://dask-ml.readthedocs.io/en/latest/examples/xgboost.html\n",
    "* Recorded screencast stepping through a similar example: https://www.youtube.com/watch?v=Cc4E-PdDSro "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
