{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is similar to the example at https://dask-ml.readthedocs.io/en/latest/examples/xgboost.html, but this example takes a while to run because it uses a real dataset. There is a recorded screencast that walks through this example  at https://www.youtube.com/watch?v=Cc4E-PdDSro, with comments from the lead Dask developer, Matthew Rocklin.\n",
    "\n",
    "This example is similar but uses synthetic data instead, to allow it to be easily runnable. This example uses a Dask wrapper around XGBoost.\n",
    "\n",
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\" width=\"30%\" alt=\"Dask logo\"> <img src=\"https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/logo-m/xgboost.png\" width=\"25%\" alt=\"Dask logo\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dask import compute, persist\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client(os.environ.get(\"DISTRIBUTED_ADDRESS\"))  # connect to cluster \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a bunch of synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "n, d = int(100e3), 20\n",
    "da.random.seed(42)\n",
    "X = da.random.normal(size=(n, d), chunks=n // 100)\n",
    "w_star = da.random.uniform(size=d, chunks=d)\n",
    "w_star = w_star**3\n",
    "y = da.sign(X @ w_star)\n",
    "\n",
    "# for binary:logistic objective, only [0, 1] labels allowed\n",
    "y = (y + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's separate into a training set and testing set, which will allow for good evaluation after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try and predict from this data with [dask-xgboost][dxgb]:\n",
    "\n",
    "[dxgb]:https://github.com/dask/dask-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_xgboost as dxgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dask-xgboost is a small wrapper around xgboost, and will behave the same as xgboost.\n",
    "\n",
    "During training it will take care of handling the data, and allow xgboost to use it's own distributed scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'objective': 'binary:logistic', 'nround': 1000, \n",
    "          'max_depth': 5, 'eta': 0.01, 'subsample': 0.5, \n",
    "          'min_child_weight': 0.5}\n",
    "\n",
    "bst = dxgb.train(client, params, X_train, y_train)\n",
    "bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bst` object is a regular `xboost.Booster` object, and has all the familar methods available. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "if isinstance(w_star, da.Array):\n",
    "    w_star = w_star.compute()\n",
    "idx_top = np.argsort(w_star)[-7:]\n",
    "top = w_star[idx_top]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12, 8), ncols=2)\n",
    "axs[0] = xgb.plot_importance(bst, ax=axs[0], height=0.8, max_num_features=7)\n",
    "axs[0].grid(False, axis=\"y\")\n",
    "axs[0].set_title('Estimated feature importance')\n",
    "\n",
    "df = pd.DataFrame({'feature_importance': top}, index=idx_top)\n",
    "df.plot.bar(ax=axs[1])\n",
    "axs[1].set_title('Ground truth feature importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can plot the ROC curve after we do some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = dxgb.predict(client, bst, X_test)\n",
    "y_hat = client.persist(y_hat)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "fpr, tpr, _ = roc_curve(y_test, y_hat)\n",
    "ax.plot(fpr, tpr, lw=3,\n",
    "        label='ROC Curve (ares = {:.2f})'.format(auc(fpr, tpr)))\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "ax.set(\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    title=\"ROC Curve\",\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    ")\n",
    "ax.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
