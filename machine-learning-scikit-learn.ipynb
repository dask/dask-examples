{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn + Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a popular machine learning library, and implements the minimal API required for training machine learning models. Their library encompasses a wide variety of problem formulations, ranging from simple linear models to more complex clustering or decomposition formulations. Scikit-learn is hindered by computation (what machine library isn't?). This has even gotten a page in their user guide, \"[Strategies to scale computationally: bigger data][strat]\".\n",
    "\n",
    "Using scikit-learn estimators with Dask-ML helps scale to large datasets. All the computation remains on scikit-learn's shoulders but the data management is handled by Dask. This allows scaling to large datasets distributed across many machines, or to datasets that do not fit in memory.\n",
    "\n",
    "This example wraps a scikit-learn estimator with a Dask-ML estimator suited for incremental learning. This Dask-ML estimator manages feeding data to Scikit-learn, and leaves the computation to Scikit-learn. This example will not cover the other algorithms inside of Dask-ML that are well-suited for Dask's distributed architecture.\n",
    "\n",
    "This example will show\n",
    "\n",
    "* wrapping a scikit-learn estimator that implement `partial_fit` with `dask_ml`\n",
    "* training, predicting, and scoring on this wrapped classifier\n",
    "* integration with other parts of sklearn (e.g., with GridSearchCV)\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\">\n",
    "<img src=\"https://www.continuum.io/sites/default/files/dask_stacked.png\" width=\"100px\">\n",
    "\n",
    "[strat]:http://scikit-learn.org/stable/modules/scaling_strategies.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data creation\n",
    "We will create many synthetic data. We are more interested in showing the plumbing this data can flow through than cool insights we can glean from the data.\n",
    "\n",
    "Let's create some synthetic data that's not too large (so it runs in a reasonable time for you) and but is realistically large. We have 100,000 examples and 100 features in this dataset we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "n, d = int(100e3), 100\n",
    "\n",
    "X = da.random.normal(size=(n, d), chunks=n // 10)\n",
    "w_star = da.exp(da.random.uniform(size=d, chunks=d))\n",
    "w_star = w_star**4\n",
    "noise = da.random.normal(size=n, chunks=n) * d / 1\n",
    "y = da.sign(X @ w_star + noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in evaluating this model, so we don't want to learn from test data (ever hear of teaching to the test?). This prevents against that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an sklearn model as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "est = SGDClassifier(loss='log', penalty='l1', tol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's wrap it with `dask_ml.wrappers.Incremental`. This will allow this model to scale well to larger datasets that Dask holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "inc = Incremental(est, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model (SGDClassifier here) must implement `partial_fit` to be used with `Incremental`. A list of models that implement this API is available on a scikit-learn documentation page under \"Incremental learning\": http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning\n",
    "\n",
    "`Incremental` does data management: it calls `est.partial_fit` on each chunk of the passed data. Dask moves the model to different chunks of the data (or vice versa) to let this happen. Note: Dask-ML `Incremental` gives data to Scikit-learn and does not change the underlying optimization algorithm that Scikit-learn uses. There are different optimization algorithms in Dask-ML that do this, especially for clustering and matrix decomposition.\n",
    "\n",
    "It is important to specify the scoring parameter in `Incremental`; otherwise, scikit-learn scorers are fed Dask arrays (which they're not optimized for).\n",
    "\n",
    "\n",
    "[1]:http://scikit-learn.org/stable/modules/scaling_strategies.html\n",
    "[inc]:http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "For an `Incremental` model, `partial_fit` and `fit` aliased to the same item and do one complete pass over the dataset. Let's call `partial_fit` a couple times, and score it after it's called each time (which will allow for a visualization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes about 1min\n",
    "\n",
    "from distributed.metrics import time\n",
    "data = []\n",
    "start = time()\n",
    "for iteration in range(10):\n",
    "    inc.partial_fit(X_train, y_train, classes=da.unique(y))\n",
    "    data += [{'score': inc.score(X_test, y_test),\n",
    "             'epochs': iteration + 1,\n",
    "             'time': time() - start}]\n",
    "    print(data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(data)\n",
    "df.plot(x='epochs', y='score')\n",
    "plt.title('Score over iterations')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting\n",
    "Let's predict, than compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = inc.predict(X_test)\n",
    "errors = np.abs(y_hat - y_test) / 2   # divide by 2 to get 0-1 labels\n",
    "accuracy = 1 - errors.mean()\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy might be the best possible, but there a lot of parameters to feed `SGDClassifier` that can drastically change the performance. We'll consider two of the most basic parameters, specifying a different loss function and different amount of penalty. This is the problem of hyperparameter optimization, something Dask-ML supports: https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html\n",
    "\n",
    "One algorithm that Dask-ML implements a grid search algorithm through `GridSearchCV`. This algorithm takes in a list of values to evaluate and score each model at every possible combination on values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import GridSearchCV\n",
    "params = {'alpha': np.logspace(-6, 0, num=10),\n",
    "          'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']}\n",
    "grid = GridSearchCV(est, params, return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we pass the scikit-learn estimator here. `Incremental` is only a small wrapper for data management, and `GridSearchCV` is also a dask tool that for data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# took about 3 minutes with 8 workers\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best loss, let's see how the score changed over different regularization constants `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(grid.cv_results_)\n",
    "show = df[df.param_loss == grid.best_params_['loss']]\n",
    "show.plot(x='param_alpha', y='mean_test_score', yerr='std_test_score',\n",
    "          logx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if other losses were equally good? Let's see a visualization of the different losses *and* different regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "show = df.pivot_table(index='param_loss', columns='param_alpha', values='mean_test_score')\n",
    "show.columns = ['%0.3f' % np.log10(x) for x in show.columns]\n",
    "show.columns.name = 'log10(param_alpha)'\n",
    "\n",
    "sns.heatmap(show, fmt='0.2f', cmap='magma', cbar_kws={'label': 'score'})\n",
    "_ = plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
