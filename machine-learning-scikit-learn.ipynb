{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn + Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a popular machine learning library whose API is easy to use because it has been inspired by other machine learning libraries. It encompasses a wide variety of problem formulations, ranging from simple linear models to more complex clustering or decomposition formulations. Scikit-learn works great for problems that fit on a single machine, but struggles for larger problems. This has even gotten a page in their user guide, \"[Strategies to scale computationally: bigger data][strat]\".\n",
    "\n",
    "Using scikit-learn estimators with Dask-ML helps scale to large datasets. All the computation remains on scikit-learn's shoulders but the data management is handled by Dask. This allows scaling to large datasets distributed across many machines, or to datasets that do not fit in memory.\n",
    "\n",
    "This example wraps a scikit-learn estimator with a Dask-ML estimator suited for incremental learning: https://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.wrappers.Incremental.html.\n",
    "\n",
    "This example will show\n",
    "\n",
    "* wrapping an estimator that implements `partial_fit` with `dask_ml`\n",
    "* training, predicting, and scoring on this wrapped classifier\n",
    "* integration with other parts of scikit-learn (e.g., with GridSearchCV)\n",
    "\n",
    "Though this example is made with a scikit-learn estimator, it will work for any class that implements the `partial_fit` and the [scikit-learn base estimator API].\n",
    "\n",
    "[scikit-learn base estimator API]:http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\">\n",
    "<img src=\"https://www.continuum.io/sites/default/files/dask_stacked.png\" width=\"100px\">\n",
    "\n",
    "[strat]:http://scikit-learn.org/stable/modules/scaling_strategies.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a distributed scheduler will provide good feedback because we can view the dask.distributed dashboard. This will provide progress and performance metrics, including visualization of jobs that are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not necessary but is encouraged because it gives feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data creation\n",
    "We will create synthetic data because we are more interested in showing how these data flow through the estimator, rather than any cool insights we can glean from the data.\n",
    "\n",
    "Let's create a synthetic dataset that's not too large (so it runs in a reasonable time) and but is realistically large and can show Dask features. We have 100,000 examples and 100 features in this dataset we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask_ml.datasets import make_classification\n",
    "import dask\n",
    "\n",
    "n, d = 100000, 100\n",
    "\n",
    "X, y = make_classification(n_samples=n, n_features=d,\n",
    "                           chunks=n // 10)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this were a real use case, the Dask arrays `X` and `y` would be real data (e.g., from file system or a database). There are a plethora of methods to create Dask arrays (https://dask.pydata.org/en/latest/array-creation.html) and Dask dataframes (https://dask.pydata.org/en/latest/dataframe-create.html).\n",
    "\n",
    "We are interested in evaluating this model, so we don't want to learn from test data (ever hear of teaching to the test?). This prevents against that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't done any work yet – Dasks operations are lazy, so we've only described to Dask what we want to do. When we tell Dask we want to do work, it'll be done in parallel on a worker or out-of-core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the underlying scikit-learn estimator as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "est = SGDClassifier(loss='log', penalty='l1', tol=1e-3, average=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's wrap it with `dask_ml.wrappers.Incremental`. `Incremental` is a meta-estimator, or an estimator that takes in another estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "inc = Incremental(est, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Incremental` does data management: it calls `est.partial_fit` on each chunk of the passed data. Dask moves the model to different chunks of the data (or vice versa) to let this happen. Note: Dask-ML `Incremental` gives data to Scikit-learn and does not change the underlying optimization algorithm that Scikit-learn uses. There are different optimization algorithms in Dask-ML that do this, especially for clustering and matrix decomposition.\n",
    "\n",
    "Our model (SGDClassifier here) must implement `partial_fit` to be used with `Incremental`. A list of models that implement this API is available on a scikit-learn documentation page under \"Incremental learning\": http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning\n",
    "\n",
    "It is important to specify the scoring parameter in `Incremental`; otherwise, scikit-learn scorers are fed Dask arrays (which they're not optimized for).\n",
    "\n",
    "\n",
    "[1]:http://scikit-learn.org/stable/modules/scaling_strategies.html\n",
    "[inc]:http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Of course, `Incremental` implements a `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.fit(X_train, y_train, classes=da.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit` will perform one loop over the dataset, calling `partial_fit` over each chunk in the Dask array.\n",
    "\n",
    "`fit` and `partial_fit` are aliased to the same function that trains over one complete pass over the dataset. Let's call `partial_fit` many times, and do 20 passes over the dataset. We'll have to create our estimators `est` and `inc` because they've already been fitted through above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import sklearn\n",
    "\n",
    "est = SGDClassifier(loss='log', penalty='l1', tol=1e-3, average=True)\n",
    "inc_delayed = delayed(Incremental(est, scoring='accuracy'))\n",
    "\n",
    "delayed_data = []\n",
    "for epoch in range(40):\n",
    "    inc = delayed(inc_delayed.partial_fit)(\n",
    "        X_train, y_train, classes=da.unique(y)\n",
    "    )\n",
    "    inc_delayed = delayed(inc)\n",
    "    delayed_data += [{'epoch': epoch,\n",
    "                      'test_score': delayed(inc.score)(X_test, y_test)}]\n",
    "inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inc, data = dask.compute(inc, delayed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for-loop is simple, even though it's doing a fairly complex operation. One could imagine having each iteration of this for-loop to be complicated and manage data IO as well as giving the estimator different chunks. With distributed, the IO cost is paid once at the beginning then not once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.plot(x='epoch', y='test_score')\n",
    "_ = plt.ylabel('Test score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call `Incremental.score` to compute the accuracy for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the accuracy ourselves by using the raw dask arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = inc.predict(X_test)\n",
    "errors = np.abs(y_pred - y_test)\n",
    "accuracy = 1 - errors.mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a lot of parameters to feed `SGDClassifier`, and this accuracy depends on those parameters, and it may not be as high as it can be. We'll consider two of the most basic parameters, specifying a different loss function and different amount of penalty. This is the problem of hyperparameter optimization, something Dask-ML supports: https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html\n",
    "\n",
    "One algorithm that Dask-ML implements a grid search algorithm through `GridSearchCV`. This algorithm takes in a list of values to evaluate and score each model at every possible combination on values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "# small parameter grid for the example\n",
    "# params = {\n",
    "#     'alpha': [0.01, 0.1, 1],\n",
    "#     'loss': ['hinge', 'log'],\n",
    "# }\n",
    "\n",
    "# An example of a larger parameter grid.\n",
    "params = {\n",
    "   'alpha': np.logspace(-6, 1, num=8),\n",
    "   'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(est, params, return_train_score=False)\n",
    "params['alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we pass the scikit-learn estimator here. `Incremental` is only a small wrapper for data management, and `GridSearchCV` is also a dask tool that for data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes about 3 minutes with 8 workers\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best loss, let's see how the score changed over different regularization constants `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(grid.cv_results_)\n",
    "show = df[df.param_loss == grid.best_params_['loss']]\n",
    "show.plot(x='param_alpha', y='mean_test_score', yerr='std_test_score',\n",
    "          logx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if other losses were equally good? Let's see a visualization of the different losses *and* different regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.factorplot(x='param_alpha', y='mean_test_score', hue='param_loss',\n",
    "               data=df,\n",
    "               aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
