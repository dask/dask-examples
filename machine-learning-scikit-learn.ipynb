{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn estimators with Dask-ML helps scale to large datasets. All the computation remains on scikit-learn's shoulders but the data management is handled by Dask. This allows scaling to large datasets distributed across many machines, or to datasets that do not fit in memory.\n",
    "\n",
    "This example shows a particular wrapping of a scikit-learn object with `Incremental`. There are other algorithms inside Dask-ML that follow the scikit-learn API but take advantage of Dask's distributed architecture for the optimization algorithms, especially with clustering or matrix decomposition (e.g., spectral clustering and PCA respectively).\n",
    "\n",
    "This example will show\n",
    "\n",
    "* wrapping a scikit-learn estimator that implement `partial_fit` with `dask_ml`\n",
    "* training, predicting, and scoring on this wrapped classifier\n",
    "* integration with other parts of sklearn (e.g., with GridSearchCV)\n",
    "\n",
    "## Getting start with Dask\n",
    "First, we create the distributed scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data creation\n",
    "We are concerned with the plumbing for parallel or distributed training is in place on somewhat realistic datasets. This means that we will create synethic data.\n",
    "\n",
    "Let's create some synthetic data that's not too large (so it runs in a reasonable time for you) and is realistically large. We have 100,000 examples and 100 features in this (synthetic) dataset we create.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "n, d = int(100e3), 100\n",
    "\n",
    "X = da.random.normal(size=(n, d), chunks=n // 10)\n",
    "w_star = da.exp(da.random.uniform(size=d, chunks=d))\n",
    "w_star = w_star**4\n",
    "noise = da.random.normal(size=n, chunks=n) * d / 1\n",
    "y = da.sign(X @ w_star + noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an sklearn model as normal than wrap it with `dask_ml.wrappers.Incremental`. This is fairly straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import Incremental\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "est = SGDClassifier(loss='log', penalty='l1', tol=1e-3)\n",
    "inc = Incremental(est, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to specify the scoring parameter in `Incremental`; otherwise, scikit-learn scorers are fed Dask arrays (which they're not optimized for).\n",
    "\n",
    "Our model (SGDClassifier here) must implement `partial_fit` to be used with `Incremental`.\n",
    "\n",
    "`Incremental` does data management: it calls `est.partial_fit` on each chunk of the passed data. It's not clear how to let `est` use parallel algorithms that look at all the data at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "For an `Incremental` model, `partial_fit` and `fit` aliased to the same item and do one complete pass over the dataset. Let's call `partial_fit` a couple times, and score it after it's called each time (which will allow for a visualization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes about 1min with 8 workers\n",
    "from distributed.metrics import time\n",
    "data = []\n",
    "start = time()\n",
    "for iteration in range(10):\n",
    "    inc.partial_fit(X_train, y_train, classes=da.unique(y))\n",
    "    data += [{'score': inc.score(X_test, y_test),\n",
    "             'epochs': iteration + 1,\n",
    "             'time': time() - start}]\n",
    "    print(data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(data)\n",
    "df.plot(x='epochs', y='score')\n",
    "plt.title('Score over iterations')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = inc.predict(X_test)\n",
    "errors = np.abs(y_hat - y_test) / 2\n",
    "accuracy = 1 - errors.mean()\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy we obtained is good, but not great. Specifying a different loss function and different amount of penalty might help.\n",
    "\n",
    "Let's do a grid search to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import GridSearchCV\n",
    "params = {'alpha': np.logspace(-6, 0, num=10),\n",
    "          'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']}\n",
    "grid = GridSearchCV(est, params, return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we pass the scikit-learn estimator here. `Incremental` is only a small wrapper for data management, and `GridSearchCV` is also a dask tool that manges data management, so this isn't a huge deal: all the core items happen inside the scikit-learn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# took about 3 minutes with 8 workers\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot a heatmap to see how these two parameters influences the prediction scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(grid.cv_results_)\n",
    "show = df.pivot_table(index='param_loss', columns='param_alpha', values='mean_test_score')\n",
    "show.columns = ['%0.3f' % np.log10(x) for x in show.columns]\n",
    "show.columns.name = 'log10(param_alpha)'\n",
    "\n",
    "sns.heatmap(show, fmt='0.2f', cmap='magma')\n",
    "_ = plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
