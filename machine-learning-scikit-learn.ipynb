{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental training of large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to training large datasets is to incrementally train a model on many batches of data sequentially.  The Scikit-Learn documentation discusses this approach in more depth in their user guide:, \"[Strategies to scale computationally: bigger data](http://scikit-learn.org/stable/modules/scaling_strategies.html)\".\n",
    "\n",
    "Many Scikit-Learn estimators implement a `partial_fit` method to enable incremental learning in batches.  Typically it is up to the user to collect batches of data to give to a Scikit-Learn estimator, which then updates a single estimator.\n",
    "\n",
    "```python\n",
    "est = SGDClassifier()\n",
    "\n",
    "for filename in filenames:\n",
    "    df = pandas.read_csv(filename)\n",
    "    x, y = ...  # get data from batch of data\n",
    "    est.partial_fit(x, y)\n",
    "```\n",
    "\n",
    "This notebook demonstrates the use of Dask-ML's `Incremental` meta-estimator, which automates this process, and improves integration with other Dask collections, changing the workflow above to something like the following:\n",
    "\n",
    "```python\n",
    "est = SGDClassifier()\n",
    "inc = Incremental(est)\n",
    "\n",
    "df = dd.read_csv(filenames)\n",
    "x = ...\n",
    "inc.fit(x)\n",
    "```\n",
    "\n",
    "Scikit-Learn handles all of the computation while Dask handles the data management, loading and moving batches of data as necessary. This allows scaling to large datasets distributed across many machines, or to datasets that do not fit in memory, all with a familiar workflow.\n",
    "\n",
    "This example will show the following:\n",
    "\n",
    "* wrapping a scikit-learn estimator that implements `partial_fit` with [dask_ml.wrappers.Incremental](https://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.wrappers.Incremental.html#dask_ml.wrappers.Incremental)\n",
    "* training, predicting, and scoring on this wrapped classifier\n",
    "\n",
    "Although this example is made with the Scikit-Learn SGDClassifer, it will work for any class that implements `partial_fit` and the [scikit-learn base estimator API].\n",
    "\n",
    "[scikit-learn base estimator API]:http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\">\n",
    "<img src=\"https://www.continuum.io/sites/default/files/dask_stacked.png\" width=\"100px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a distributed scheduler will provide good feedback because we can view the dask.distributed dashboard. This will provide progress and performance metrics, including visualization of jobs that are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33843\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>8.35 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:33843' processes=4 cores=4>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data creation\n",
    "\n",
    "We create a synthetic dataset that is large enough to be interesting, but small enough to run quickly.  It has 100,000 examples and 100 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array<normal, shape=(1000000, 100), dtype=float64, chunksize=(10000, 100)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask_ml.datasets import make_classification\n",
    "import dask\n",
    "\n",
    "n, d = 1000000, 100\n",
    "\n",
    "X, y = make_classification(n_samples=n, n_features=d,\n",
    "                           chunks=n // 100)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on creating dask arrays and dataframes from real data, see documentation on [Dask arrays](https://dask.pydata.org/en/latest/array-creation.html) and [Dask dataframes](https://dask.pydata.org/en/latest/dataframe-create.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data for training and testing\n",
    "\n",
    "To aid evaluation we split our dataset into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array<concatenate, shape=(900000, 100), dtype=float64, chunksize=(9000, 100)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist data in memory\n",
    "\n",
    "This dataset is small enough to fit in distributed memory, so we call `dask.persist` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = dask.persist(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working in a situation where your dataset does not fit in memory then you should skip this step.  Everything will still work, but will be slower and use less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute classes\n",
    "\n",
    "We pre-compute the classes from our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = da.unique(y_train).compute()\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scikit-Learn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the underlying Scikit-Learn estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "est = SGDClassifier(loss='log', penalty='l1', tol=1e-3, average=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `SGDClassifier`, but any estimator that implements the `partial_fit` method will work.  A list of Scikit-Learn models that implement this API is available [here](http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap with Dask-ML's Incremental meta-estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wrap our `SGDClassifer` with the [`dask_ml.wrappers.Incremental`](https://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.wrappers.Incremental.html#dask_ml.wrappers.Incremental) meta-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "inc = Incremental(est, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `Incremental` only does data management while leaving the actual algorithm to the underlying Scikit-Learn estimator.\n",
    "\n",
    "Note: If using Dask arrays for testing data it is helpful to specify the scoring parameter in `Incremental`; otherwise, Scikit-Learn scorers are fed Dask arrays, for which they are not well optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Incremental` implements a `fit` method, which will perform one loop over the dataset, calling `partial_fit` over each chunk in the Dask array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Incremental(SGDClassifier(alpha=0.0001, average=True, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l1', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=0.001, verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc.fit(X_train, y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67499"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass over the training data many times\n",
    "\n",
    "Calling `.fit` passes over all chunks our data once.  However, in many cases we may want to pass over the training data many times.  To do this we can use the `Incremental.partial_fit` method and a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = SGDClassifier(loss='hinge', penalty='l2', tol=0e-3, average=True)\n",
    "inc = Incremental(est, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.67675\n",
      "Score: 0.67675\n",
      "Score: 0.67675\n",
      "Score: 0.67675\n",
      "Score: 0.67675\n",
      "Score: 0.67675\n",
      "Score: 0.67675\n",
      "Score: 0.67675\n",
      "Score: 0.67675\n",
      "Score: 0.67675\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    inc.partial_fit(X_train, y_train, classes=classes)\n",
    "    print('Score:', inc.score(X_test, y_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score here may not change much on our synthetic dataset.  This is a sign that we converged early on.  This may differ though in practice on real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can also call `Incremental.predict` and `Incremental.score` on our testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array<predict, shape=(100000,), dtype=int64, chunksize=(1000,)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc.predict(X_test)  # Predict produces lazy dask arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc.predict(X_test)[:100].compute()  # call compute to get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67675"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "In this notebook we went over using Dask-ML's `Incremental` meta-estimator to automate the process of incremental training with Scikit-Learn estimators that implement the `partial_fit` method.  If you want to learn more about this process you might want to investigate the following documentation:\n",
    "\n",
    "1.  http://scikit-learn.org/stable/modules/scaling_strategies.html\n",
    "2.  [Dask-ML Incremental API documentation](https://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.wrappers.Incremental.html#dask_ml.wrappers.Incremental)\n",
    "3.  [List of Scikit-Learn estimators compatible with incremental learning](http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
